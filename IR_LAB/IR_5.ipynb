{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ed8e06-0cf2-4264-b4e4-fb6b89b2de73",
   "metadata": {},
   "source": [
    "### Implement Agglomerative hierarchical clustering algorithm using appropriate dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a877073-bc1b-421e-8862-dee49cf24e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank values:\n",
      "{0: 0.028225221344340273, 1: 0.01337258610187707, 2: 0.01278019140055241, 3: 0.013175697960476889, 4: 0.01298132107058749, 5: 0.01318781361064749, 6: 0.012377905435756842, 7: 0.0127717119774061, 8: 0.013157283612231835, 9: 0.013178031011730873, 10: 0.012979380531714673, 11: 0.013372371204340127, 12: 0.012767909599195675, 13: 0.012762964863236108, 14: 0.012982995063759283, 15: 0.01296423613610792, 16: 0.013364899784992888, 17: 0.012982148011506392, 18: 0.013370567578156552, 19: 0.013374270901695701, 20: 0.013574929613221515, 21: 0.012353148549986865, 22: 0.012968749481791919, 23: 0.012767968029910226, 24: 0.012792134401655936, 25: 0.011181235754962961, 26: 0.011974401516145568, 27: 0.01317088017684138, 28: 0.013170047860868445, 29: 0.01357363994514994, 30: 0.013379513103028642, 31: 0.012964022020484682, 32: 0.012362002508011205, 33: 0.01297217679447552, 34: 0.01255903225224812, 35: 0.012574353188341741, 36: 0.013584516803513346, 37: 0.013574929613221515, 38: 0.012369706549311585, 39: 0.013162119089422565, 40: 0.013581856614373087, 41: 0.013168371289021197, 42: 0.02753721089486636, 43: 0.02711541179366847, 44: 0.02676857721795108, 45: 0.02614755391711983, 46: 0.02619253491517836, 47: 0.02594968154524775, 48: 0.02529075631727838, 49: 0.024556110778428477, 50: 0.024786226036336376, 51: 0.02420173893376831, 52: 0.023674427628363765, 53: 0.023585521814959105, 54: 0.02302600253458119, 55: 0.022880772307385125, 56: 0.022695267968410905, 57: 0.02226896400606871, 58: 0.021886460823625806, 59: 0.021527538210461453}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Custom PageRank function\n",
    "def pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1.0e-6, nstart=None, weight='weight', dangling=None):\n",
    "    if len(G) == 0: \n",
    "        return {} \n",
    "  \n",
    "    # Convert to directed graph if G is undirected\n",
    "    if not G.is_directed(): \n",
    "        D = G.to_directed() \n",
    "    else: \n",
    "        D = G\n",
    "        \n",
    "    # Create a copy in (right) stochastic form \n",
    "    W = nx.stochastic_graph(D, weight=weight) \n",
    "    N = W.number_of_nodes()\n",
    "    \n",
    "    # Choose fixed starting vector if not given \n",
    "    if nstart is None: \n",
    "        x = dict.fromkeys(W, 1.0 / N) \n",
    "    else: \n",
    "        # Normalized nstart vector \n",
    "        s = float(sum(nstart.values())) \n",
    "        x = dict((k, v / s) for k, v in nstart.items()) \n",
    "        \n",
    "    # Personalization vector setup\n",
    "    if personalization is None:\n",
    "        # Assign uniform personalization vector if not given \n",
    "        p = dict.fromkeys(W, 1.0 / N)\n",
    "    else: \n",
    "        missing = set(G) - set(personalization) \n",
    "        if missing: \n",
    "            raise nx.NetworkXError('Personalization dictionary must have a value for every node. Missing nodes %s' % missing) \n",
    "        s = float(sum(personalization.values())) \n",
    "        p = dict((k, v / s) for k, v in personalization.items())\n",
    "        \n",
    "    # Dangling nodes handling\n",
    "    if dangling is None:\n",
    "        # Use personalization vector if dangling vector not specified \n",
    "        dangling_weights = p \n",
    "    else: \n",
    "        missing = set(G) - set(dangling) \n",
    "        if missing: \n",
    "            raise nx.NetworkXError('Dangling node dictionary must have a value for every node. Missing nodes %s' % missing) \n",
    "        s = float(sum(dangling.values())) \n",
    "        dangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
    "        \n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "    \n",
    "    # Power iteration: make up to max_iter iterations \n",
    "    for _ in range(max_iter): \n",
    "        xlast = x \n",
    "        x = dict.fromkeys(xlast.keys(), 0) \n",
    "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes) \n",
    "        for n in x:\n",
    "            # Matrix multiply with left multiply x^T=xlast^T*W \n",
    "            for nbr in W[n]: \n",
    "                x[nbr] += alpha * xlast[n] * W[n][nbr][weight] \n",
    "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n] \n",
    "  \n",
    "        # Check convergence, L1 norm \n",
    "        err = sum([abs(x[n] - xlast[n]) for n in x]) \n",
    "        if err < N * tol: \n",
    "            return x \n",
    "    raise nx.NetworkXError('Pagerank: power iteration failed to converge in %d iterations.' % max_iter)\n",
    "\n",
    "# Generate a test graph\n",
    "G = nx.barabasi_albert_graph(60, 41)  # Creates a Barabási–Albert graph with 60 nodes\n",
    "\n",
    "# Calculate PageRank using the custom function\n",
    "pr = pagerank(G, alpha=0.4)  # Using alpha=0.4 as specified in your example\n",
    "\n",
    "# Print the resulting PageRank values\n",
    "print(\"PageRank values:\")\n",
    "print(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48493137-7db1-4b89-ace3-bac92656321c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
